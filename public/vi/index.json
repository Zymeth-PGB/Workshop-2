[
{
	"uri": "/vi/1-introduce-service/",
	"title": "Giới thiệu các dịch vụ và công cụ",
	"tags": [],
	"description": "",
	"content": "Các dịch vụ và công cụ được sử dụng Amazon S3 (Simple Storage Service) là một dịch vụ lưu trữ đối tượng trên đám mây của Amazon Web Services (AWS), cho phép bạn lưu trữ và truy xuất bất kỳ lượng dữ liệu nào, từ bất kỳ đâu trên web, vào bất kỳ thời điểm nào. S3 được thiết kế để cung cấp khả năng lưu trữ có độ bền cao, dễ dàng mở rộng quy mô. AWS Glue là một dịch vụ ETL (Extract, Transform, Load) không máy chủ của Amazon Web Services (AWS), giúp bạn chuẩn bị và xử lý dữ liệu một cách dễ dàng. AWS Glue tự động hóa quy trình trích xuất dữ liệu từ các nguồn khác nhau, biến đổi dữ liệu và tải chúng vào các kho lưu trữ dữ liệu hoặc dịch vụ phân tích khác. Amazon Athena là một dịch vụ truy vấn SQL không máy chủ do Amazon Web Services (AWS) cung cấp, cho phép người dùng phân tích dữ liệu trực tiếp trên Amazon S3 bằng cách sử dụng ngôn ngữ SQL chuẩn mà không cần phải thiết lập hoặc quản lý cơ sở hạ tầng. AWS Athena đặc biệt phù hợp cho việc phân tích dữ liệu lớn được lưu trữ trong S3 mà không cần phải tải hoặc di chuyển dữ liệu sang một hệ thống khác. Amazon Redshift là một dịch vụ kho dữ liệu đám mây (cloud data warehouse) của Amazon Web Services (AWS), được thiết kế để phân tích dữ liệu quy mô lớn và hỗ trợ truy vấn SQL với tốc độ cao. Redshift cho phép bạn lưu trữ và phân tích dữ liệu từ nhiều nguồn khác nhau, từ đó giúp doanh nghiệp xây dựng các hệ thống phân tích, báo cáo và hỗ trợ ra quyết định. AWS IAM (Identity and Access Management) là một dịch vụ của Amazon Web Services (AWS) giúp quản lý quyền và kiểm soát truy cập cho các tài nguyên và dịch vụ trong AWS. IAM cho phép bạn kiểm soát ai có thể truy cập vào tài nguyên AWS (người dùng, nhóm người dùng hoặc dịch vụ) và những hành động nào họ có thể thực hiện trên các tài nguyên đó. Grafana là một công cụ mã nguồn mở dùng để giám sát, trực quan hóa dữ liệu và tạo các bảng điều khiển (dashboard) tương tác từ nhiều nguồn dữ liệu khác nhau. Grafana thường được sử dụng để theo dõi hiệu suất hệ thống, phân tích log, và hiển thị các số liệu theo thời gian thực, cho phép người dùng dễ dàng tạo ra các biểu đồ và bảng điều khiển trực quan nhằm giúp phân tích và giám sát dữ liệu một cách hiệu quả. "
},
{
	"uri": "/vi/3-preparation-step/1-create-s3/",
	"title": "Tạo Bucket S3",
	"tags": [],
	"description": "",
	"content": "Tạo bucket chứa Raw Data Nhập và tìm S3 trong thanh tìm kiếm. Sau đó ấn chọn như hình. Chọn Buckets ở thanh điều hướng bên trái và chọn Create Bucket. Đặt tên Bucket trong ô được khoanh đỏ trong hình. Các tùy chọn khác giữ mặc định và chọn Create Bucket. Khi tạo thành công sẽ có dòng thông báo xanh ở trên và thấy nó xuất hiện trong danh sách các Bucket hiện có của mình. "
},
{
	"uri": "/vi/",
	"title": "Tạo một Data Pipeline đơn giản từ các dịch vụ AWS",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phạm vi bài lab này bạn sẽ tìm hiểu về một Data Pipeline đơn giản từ việc dữ liệu vào và xử lý như thế nào để đưa ra được kết quả đến người dùng. Ở đây ta sẽ đưa kết quả ra dạng các biểu đồ thông qua Grafana hoặc sẽ lọc những thông tin cần thiết từ AWS Athena.\nNội dung chính Giới thiệu các dịch vụ và công cụ Chuẩn bị dữ liệu Thiết lập các dịch vụ Cài đặt và kết nối với Grafana Dọn dẹp tài nguyên "
},
{
	"uri": "/vi/2-eda-data/",
	"title": "Chuẩn bị dữ liệu",
	"tags": [],
	"description": "",
	"content": "Nguồn dữ liệu Dữ liệu được sử dụng trong workshop này có ngữ cảnh là Một công ty viễn thông hư cấu đã cung cấp dịch vụ điện thoại gia đình và Internet cho 7043 khách hàng ở California trong quý 3.\nCó thể tìm và tải dữ liệu ở đây : Telco customer churn: IBM dataset\nKhám phá dữ liệu Mô tả các trường dữ liệu :\nCustomerID: Một ID duy nhất xác định từng khách hàng. Count: Một giá trị được sử dụng trong báo cáo/bảng thông tin để tổng hợp số lượng khách hàng trong một tập hợp đã lọc. Country: Quốc gia nơi cư trú chính của khách hàng. State: Tiểu bang nơi cư trú chính của khách hàng. City: Thành phố nơi cư trú chính của khách hàng. Zip Code: Mã bưu chính nơi cư trú chính của khách hàng. Lat Long: Vĩ độ và kinh độ kết hợp của nơi cư trú chính của khách hàng. Latitude: Vĩ độ của nơi cư trú chính của khách hàng. Longitude: Kinh độ của nơi cư trú chính của khách hàng. Gender: Giới tính của khách hàng: Male, Female Senior Citizen: Chỉ ra nếu khách hàng từ 65 tuổi trở lên: Yes, No Partner: Chỉ ra nếu khách hàng có đối tác: Yes, No Dependents: Chỉ ra nếu khách hàng sống với bất kỳ người phụ thuộc nào: Yes, No. Người phụ thuộc có thể là con cái, cha mẹ, ông bà, v.v. Tenure Months: Chỉ ra tổng số tháng mà khách hàng đã làm việc cho công ty vào cuối quý được chỉ định ở trên. Phone Service: Chỉ ra nếu khách hàng đăng ký dịch vụ điện thoại gia đình với công ty: Yes, No Multiple Lines: Chỉ ra nếu khách hàng đăng ký nhiều đường dây điện thoại với công ty: Yes, No Internet Service: Chỉ ra nếu khách hàng đăng ký dịch vụ Internet với công ty: No, DSL, Fiber Optic, Cable. Online Security: Chỉ ra liệu khách hàng có đăng ký dịch vụ bảo mật trực tuyến bổ sung do công ty cung cấp hay không: Yes, No Online Backup: Chỉ ra liệu khách hàng có đăng ký dịch vụ sao lưu trực tuyến bổ sung do công ty cung cấp hay không: Yes, No Device Protection: Chỉ ra liệu khách hàng có đăng ký gói bảo vệ thiết bị bổ sung cho thiết bị Internet do công ty cung cấp hay không: Yes, No Tech Support: Chỉ ra liệu khách hàng có đăng ký gói hỗ trợ kỹ thuật bổ sung từ công ty với thời gian chờ được rút ngắn hay không: Yes, No Streaming TV: Chỉ ra liệu khách hàng có sử dụng dịch vụ Internet của mình để truyền phát chương trình truyền hình từ nhà cung cấp bên thứ ba hay không: Yes, No. Công ty không tính thêm phí cho dịch vụ này. Streaming Movies: Chỉ ra liệu khách hàng có sử dụng dịch vụ Internet của mình để truyền phát phim từ nhà cung cấp bên thứ ba hay không: Yes, No. Công ty không tính thêm phí cho dịch vụ này. Contract: Chỉ ra loại hợp đồng hiện tại của khách hàng: Month-to-Month, One Year, Two Year. Paperless Billing: Chỉ ra nếu khách hàng đã chọn thanh toán không cần giấy tờ: Yes, No Payment Method: Chỉ ra cách khách hàng thanh toán hóa đơn của họ: Bank Withdrawal, Credit Card, Mailed Check Monthly Charge: Chỉ ra tổng phí hàng tháng hiện tại của khách hàng cho tất cả các dịch vụ của công ty. Total Charges: Chỉ ra tổng phí của khách hàng, được tính đến cuối quý được chỉ định ở trên. Churn Label: Yes = khách hàng đã rời công ty trong quý này. No = khách hàng vẫn tiếp tục gắn bó với công ty. Có liên quan trực tiếp đến Churn Value. Churn Value: 1 = khách hàng đã rời công ty trong quý này. 0 = khách hàng vẫn tiếp tục gắn bó với công ty. Có liên quan trực tiếp đến Churn Label. Churn Score: Giá trị từ 0-100 được tính bằng công cụ dự đoán IBM SPSS Modeler. Mô hình kết hợp nhiều yếu tố được biết là nguyên nhân gây ra tình trạng churn. Điểm càng cao thì khả năng khách hàng churn càng cao. CLTV: Giá trị trọn đời của khách hàng. CLTV dự đoán được tính bằng cách sử dụng các công thức của công ty và dữ liệu hiện có. Giá trị càng cao, khách hàng càng có giá trị. Khách hàng có giá trị cao nên được theo dõi để biết tình trạng churn. Churn Reason: Lý do cụ thể của khách hàng khi rời khỏi công ty. Liên quan trực tiếp đến Churn Category. Bộ dữ liệu được mô tả chi tiết hơn ở đây : Telco customer churn\n"
},
{
	"uri": "/vi/3-preparation-step/2-create-iam-role/",
	"title": "Tạo IAM Role",
	"tags": [],
	"description": "",
	"content": "Tạo IAM role chung cho các dịch vụ Nhập và tìm kiếm IAM ở thanh tìm kiếm, sau đó ấn chọn vào IAM. Ở thanh điều hướng bên trái chọn Roles để truy cập vào bảng điều khiển Roles. Ấn vào Create role để tạo role Chọn AWS Service và điền Glue vào Service or use case để chọn Glue. Chọn Next để qua bước tiếp theo. Tìm và chọn AdministratorAccess sau đó ấn Next để qua bước tiếp theo. Ở bước này bạn chỉ việc điền tên và ấn Create role, các step khác vẫn để mặc định. Sau khi tạo role xong bạn có thể kiểm tra xem mình có tạo được chưa trong danh sách các role, ở đây mình đã tạo thành công như ô khoanh màu đỏ "
},
{
	"uri": "/vi/3-preparation-step/3-create-table/",
	"title": "Tạo các database để chứa dữ liệu",
	"tags": [],
	"description": "",
	"content": "Tạo Database Nhập và tìm kiếm Glue ở thanh tìm kiếm sau đó ấn chọn. Ở thanh điều hướng bên trái chọn Database ở mục Data Catalog. Chọn Add Database để thêm database. Nhập tên sau đó chọn Create Database để tạo database. Khi tạo thành công sẽ xuất hiện các database đã tạo ở phần khung đỏ Trong bước thứ 5 bạn có thể thấy ở đây khoanh vùng 2 database là do trong workshop này ta sẽ sử dụng 1 database cho việc lấy dữ liệu từ S3 và cái thứ 2 là để đưa dữ liệu sau khi transform vào Redshift. Cho nên bạn phải tạo 2 database như hình trên.\n"
},
{
	"uri": "/vi/3-preparation-step/",
	"title": "Thiết lập các dịch vụ",
	"tags": [],
	"description": "",
	"content": "Các bước thiết lập Chúng ta thực hiện theo từng bước sau đây:\n1. Tạo Bucket S3\n2. Tạo IAM Role\n3. Tạo các table để chứa dữ liệu\n4. Thiết lập và kết nối tới Redshift\n5. Tạo Crawler\n6. Chuẩn bị một quy trình ETL\n7. Thiết lập và Query với Athena\n"
},
{
	"uri": "/vi/4-set-up-grafana/",
	"title": "Cài đặt và kết nối với Grafana",
	"tags": [],
	"description": "",
	"content": "Cài đặt Grafana Có 2 cách để cài đặt Grafana là cài trực tiếp vào máy hoặc cài thông qua Docker.\nCài Grafana trực tiếp vào máy\n1.1. Truy cập vào trang download Grafana.\n1.2. Lựa chọn phiên bản phù hợp với hệ điều hành máy tính của bạn và tiến hành cài đặt.\nCài Grafana sử dụng Docker\n2.1. Truy cập vào Docker hub và tìm Grafana image.\n2.2. Sau đó mở terminal và chạy lệnh sau:\ndocker run -d --name=grafana -p 3000:3000 grafana/grafana 2.3. Kiểm tra xem đã cài đặt hay chưa sử dụng lệnh:\ndocker ps Kết nối Grafana tới Redshift Truy cập vào Grafana thông qua localhost với port là 3000. Username và password mặc định là admin và admin cho lần đầu đăng nhập. Nếu muốn thay đổi bạn có thể truy cập vào Profile và thực hiện thay đổi mật khẩu. Sau khi đăng nhập thành công ta sẽ thực hiện kết nối tới Redshift như sau: Ở thanh điều hướng bên trái chọn connection. Nhập và tìm kiếm Redshift ở thanh tìm kiếm. Chọn Install để thêm connection tới Redshift. Sau khi cài đặt thành công chọn Add new data source để thêm data đã transforms để thực hiện tạo Dashboard theo nhu cầu. "
},
{
	"uri": "/vi/3-preparation-step/4-set-up-redshift/",
	"title": "Thiết lập và kết nối tới Redshift",
	"tags": [],
	"description": "",
	"content": "Tạo Redshift Cluster Nhập và tìm kiếm Redshift trong thanh tìm kiếm sau đó ấn chọn Redshift. Ở thanh điều hướng bên trái chọn Cluster. Chọn Create Cluster để tạo một Cluster mới. Nhập tên Cluster và chọn các tùy chọn như hình, do mục tiêu là làm để tìm hiểu nên ta chọn ra3.xlplus để tiết kiệm chi phí. Nhập tên cho Admin User và chọn nhập mật khẩu thủ công để đặt mật khẩu của bạn. Username và password này được sử dụng để tạo kết nối tới Query Editor nên bạn cần ghi nhớ để có thể kết nối đến Query Editor.\nSử dụng các cài đặt bổ sung mặc định và ấn Create Cluster để tạo Cluster. Sau khi tạo xong Cluster, nhìn vào thanh điều hướng bên trái và chọn Query Editor V2. Tại màn hình Query Editor chọn vào dấu 3 chấm sau đó chọn Create connection để tạo kết nối tới Query Editor. Khi chọn Create connection sẽ hiển thị lên màn hình các lựa chọn để liên kết đến Query Editor, ta chọn như hình và điền Username, password đã tạo trước đó vào. Sau khi tạo kết nối đến Query Editor thành công ta sẽ dán Script SQL được tạo sẵn ở Github và ấn run để tạo ra table có các trường dữ liệu phù hơp với dữ liệu sẽ được load vào. Tạo thành công table thì ta sẽ quay trở lại thanh điều khiển bên trái của Glue và chọn connection. Chọn Create connection để tạo connection tới Redshift. Ở bước đầu tiên tìm kiếm data source là Redshift sau đó ấn chọn và ấn next. Chọn Database instances là cluster vừa tạo và nhập Username password, sau đó chọn next. Đặt tên cho connection và chọn next. Kiểm tra lại các cài đặt đã đúng chưa và chọn Creation connection. Tạo xong sẽ có connection tới Redshift như hình. "
},
{
	"uri": "/vi/5-clean-up/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ dọn dẹp tài nguyên theo thứ tự như sau: Xóa Bucket S3:\nTruy cập vào mục quản lý Buckets. Chọn Buckets đã tạo trong Workshop. Chọn Delete và nhập lại tên Bucket như yêu cầu để xóa Bucket. Xóa IAM role:\nTruy cập console của IAM chọn phần Roles. Chọn vào Role đã tạo trong Workshop và chọn Delete. Nhập lại tên Role và hoàn thành Delete. Xóa Databases trong Glue:\nTruy cập vào console Glue. Chọn Databases và chọn các Databases đã tạo trong Workshop. Chọn Delete để xóa các Databases. Xóa Redshift cluster và connection Redshift:\nXóa cluster\nTruy cập vào console Redshift. Chọn View all clusters để xem tất cả các cluster. Chọn cluster đã tạo và chọn Action để thực hiện Delete. Sau khi chọn Delete trong Action ta bỏ chọn Create final snapshot và điền delete vào ô để hoàn thành Delete cluster. Xóa connection\nTruy cập vào console Glue. Chọn connections ở thanh điều hướng bên trái. Chọn connection đã tạo trong Workshop và chọn Action để thực hiện Delete. Sau khi chọn Delete trong Action tiếp tục xác nhận Delete để xóa. Xóa Crawler:\nTruy cập vào console Glue. Chọn crawlers ở thanh điều hướng bên trái. Chọn crawler đã tạo trong Workshop và chọn Action để thực hiện Delete. Sau khi chọn Delete trong Action tiếp tục xác nhận Delete để xóa. Xóa ETL job:\nTruy cập vào console Glue. Chọn ETL jobs ở thanh điều hướng bên trái. Chọn job đã tạo trong Workshop và chọn Action để thực hiện Delete. Sau khi chọn Delete trong Action tiếp tục xác nhận Delete để xóa. "
},
{
	"uri": "/vi/3-preparation-step/5-create-crawler/",
	"title": "Tạo Crawler",
	"tags": [],
	"description": "",
	"content": "Tạo các Crawler Ta sẽ tạo 2 crawler để phục vụ cho mục tiêu lấy schema từ S3 và lấy schema từ Redshift đã tạo ở bước trước. Nên một số bước giống nhau sẽ chỉ để 1 ảnh còn những bước có khác biệt sẽ có hướng dẫn mỗi crawler nên làm như thế nào.\nNhập và tìm kiêm Glue trên thanh tìm kiếm. Ở thanh điều hướng bên trái chọn Crawler. Sau khi chọn Crawler sẽ hiện ra danh sách các Crawler và các tùy chọn khác. Ta chọn Create crawler để tạo crawler mới. Nhập tên cho mỗi Crawler phù hợp với chức năng của nó là lấy schema từ đâu. Bước 2 chọn Not yet và sẽ thêm một data source vào. Ở mỗi Crawler sẽ có cách lựa chọn data source khác nhau như sau:\nĐối với Crawler lấy schema từ S3 thì chọn như hình:\nĐường dẫn đến S3 ta sẽ chọn bucket được tạo ở bước số 1 Đối với Crawler lấy schema từ Redshift thì chọn như hình:\nỞ mục Include Path thì ta sẽ thêm đường dẫn đến table ta đã tạo ở bước trước trong Query Editor. Sau khi lựa chọn data source xong ta sẽ được data source như 2 hình bên dưới. Chọn next để đến với bước tiếp theo.\nCrawler S3 Crawler Redshift Chọn IAM role đã tạo ở bước số 2. Chọn database phù hợp cho mỗi Crawler đã được tạo ở bước số 3:\nCrawler S3 Crawler Redshift Kiểm tra lại các lựa chọn nếu ổn thì chọn Create crawler:\nCrawler S3 Crawler Redshift Khi tạo thành công thì sẽ được 2 crawler có State là Ready như hình. "
},
{
	"uri": "/vi/3-preparation-step/6-create-etl-flow/",
	"title": "Chuẩn bị một quy trình ETL",
	"tags": [],
	"description": "",
	"content": "Sử dụng Glue để thực hiện ETL Nhập và tìm Glue ở thanh tìm kiếm. Ở thanh điều hướng bên trái chọn ETL jobs. Chọn Visual ETL để tạo một quy trình ETL. Trong tab Visual chọn phần Sources để chọn nguồn dữ liệu, ở đây ta chọn S3. Chuyển qua phần kế là Transforms ta sẽ chọn Change Schema để thực hiện các thay đổi trên dữ liệu. Ở phần Target ta chọn Redshift để load dữ liệu vào. Sau khi chọn xong các thành phần cho quy trình ETL thì ta sẽ cấu hình ở từng phần như các hình sau:\nS3 Change Schema\nTa sẽ Drop các trường dữ liệu sau : count, country, state, lat long, latitude, longitude, cltv. Đối với các trường dữ liệu có tên tên là 2 chữ như \u0026ldquo;zip code\u0026rdquo; thì ta sẽ đổi lại thành \u0026ldquo;zip_code\u0026rdquo;. Áp dụng cho tất cả các trường dữ liệu có tên dài. Redshift Ta thay đổi tên của job và save job lại. Chuyển qua phần Job details và gán IAM role cho job này là role được tạo trong bước 2. Sau khi thực hiện xong quay lại màn hình quản lý các job ta sẽ thấy được job mới được tạo như hình. "
},
{
	"uri": "/vi/3-preparation-step/7-set-up-athena/",
	"title": "Thiết lập và query với Athena",
	"tags": [],
	"description": "",
	"content": "Thiết lập Athena Nhập và tìm kiếm Athena trong thanh tìm kiếm. Trong màn hình Query Editor của Athena chọn Data Source như hình. Phần Database sẽ hiển thị Database trước khi Transforms và sau khi Transforms, bạn có thể chọn bất kì cái nào tùy vào mục đích sử dụng, khi chọn xong thì ở phần Tables sẽ xuất hiện các Table của Database đã chọn. Để có thể chạy được các query thì ta cần phải cài đặt thêm vị trí của các kết quả query. Ở đây ta chọn phần Settings và chọn Manage. Trước khi chọn đường dẫn đến S3 thì ta phải quay lại S3 và tạo 1 Bucket phục vụ cho việc chứa kết quả query. Sau khi tạo xong thì chọn đường dẫn đến Bucket đó và chọn Save. "
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]